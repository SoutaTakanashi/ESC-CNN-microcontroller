{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from lime import lime_image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import input_data\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff 43 98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 98, 43, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def settings(sample_rate=16000, clip_duration_ms=1000.0,\n",
    "                    window_size_ms=30.0, window_stride_ms=10.0,\n",
    "                    feature_bin_count=40, preprocess='average', n_classes=12):\n",
    "\n",
    "    m = models.prepare_model_settings(n_classes, sample_rate,\n",
    "                                      clip_duration_ms, window_size_ms, window_stride_ms,\n",
    "                                      feature_bin_count, preprocess)\n",
    "    return m\n",
    "\n",
    "model_settings = settings()\n",
    "\n",
    "def wav_to_features(input_wav):\n",
    "    \n",
    "    audio_processor = input_data.AudioProcessor(None, None, 0, 0, '', 0, 0, model_settings, None)\n",
    "\n",
    "    results = audio_processor.get_features_for_wav(input_wav, model_settings, session)\n",
    "    features = results[0]\n",
    "    return features\n",
    "\n",
    "f = wav_to_features('../../data/speech_dataset/nine/122c5aa7_nohash_0.wav')\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv out 36 1 186\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "first_weights:0 (float32_ref 98x8x1x186) [145824, bytes: 583296]\n",
      "first_bias:0 (float32_ref 186) [186, bytes: 744]\n",
      "first_fc_weights:0 (float32_ref 6696x128) [857088, bytes: 3428352]\n",
      "first_fc_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "second_fc_weights:0 (float32_ref 128x128) [16384, bytes: 65536]\n",
      "second_fc_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "final_fc_weights:0 (float32_ref 128x12) [1536, bytes: 6144]\n",
      "final_fc_bias:0 (float32_ref 12) [12, bytes: 48]\n",
      "Total size of variables: 1021286\n",
      "Total bytes of variables: 4085144\n",
      "INFO:tensorflow:Restoring parameters from train/low_latency_conv.ckpt-12600\n",
      "INFO:tensorflow:Restoring parameters from train/low_latency_conv.ckpt-12600\n",
      "s (98, 43, 1) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.68610454,  0.09988894, -0.04474275,  0.2969219 , -0.18442798,\n",
       "         0.4439205 , -0.05753042,  0.03617053,  0.20074761, -0.1346468 ,\n",
       "         0.01625966,  0.00969209]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_size = (98, 43, 1)\n",
    "f_size = (None, 98, 43, 1)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "processed_images = tf.placeholder(tf.float32, shape=f_size)\n",
    "model = models.create_model(processed_images, model_settings,\n",
    "                            model_architecture='low_latency_conv', is_training=False)\n",
    "\n",
    "def transform_img_fn(path_list):\n",
    "    out = []\n",
    "    for f in path_list:\n",
    "        image = wav_to_features(f).squeeze(axis=0)\n",
    "        print('s', image.shape, type(image))\n",
    "        i = tf.convert_to_tensor(image) \n",
    "        out.append(i)\n",
    "    return session.run([out])[0]\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "#tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "checkpoint = 'train/low_latency_conv.ckpt-12600'\n",
    "\n",
    "models.load_variables_from_checkpoint(session, checkpoint)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "saver.restore(session, checkpoint)\n",
    "\n",
    "probabilities = model\n",
    "def predict_fn(paths):\n",
    "    images = transform_img_fn(paths)\n",
    "    return session.run(probabilities, feed_dict={processed_images: images})\n",
    "\n",
    "predict_fn(['../../data/speech_dataset/nine/122c5aa7_nohash_0.wav'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
