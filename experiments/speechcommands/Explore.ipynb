{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from lime import lime_image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import input_data\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff 43 98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def setup(sample_rate=16000, clip_duration_ms=1000.0,\n",
    "            window_size_ms=30.0, window_stride_ms=10.0,\n",
    "            feature_bin_count=40, preprocess='average', n_classes=12,\n",
    "            silence = 10.0, unknown = 10.0,\n",
    "            testing = 10.0, validation = 10.0):\n",
    "\n",
    "    m = models.prepare_model_settings(n_classes, sample_rate,\n",
    "                                      clip_duration_ms, window_size_ms, window_stride_ms,\n",
    "                                      feature_bin_count, preprocess)\n",
    "    \n",
    "    wanted_words = 'yes,no,up,down,left,right,on,off,stop,go'.split(',')\n",
    "    p = input_data.AudioProcessor(None, '../../data/speech_dataset/',\n",
    "                                silence, unknown, wanted_words,\n",
    "                                validation, testing, model_settings, None)\n",
    "    return m, p\n",
    "\n",
    "model_settings, audio_processor = setup()\n",
    "\n",
    "\n",
    "def wav_to_features(input_wav):\n",
    "    results = audio_processor.get_features_for_wav(input_wav, model_settings, session)\n",
    "    features = results[0]\n",
    "    return features\n",
    "\n",
    "#f = wav_to_features('../../data/speech_dataset/nine/122c5aa7_nohash_0.wav')\n",
    "#f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv out 36 1 186\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "first_weights:0 (float32_ref 98x8x1x186) [145824, bytes: 583296]\n",
      "first_bias:0 (float32_ref 186) [186, bytes: 744]\n",
      "first_fc_weights:0 (float32_ref 6696x128) [857088, bytes: 3428352]\n",
      "first_fc_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "second_fc_weights:0 (float32_ref 128x128) [16384, bytes: 65536]\n",
      "second_fc_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "final_fc_weights:0 (float32_ref 128x12) [1536, bytes: 6144]\n",
      "final_fc_bias:0 (float32_ref 12) [12, bytes: 48]\n",
      "Total size of variables: 1021286\n",
      "Total bytes of variables: 4085144\n",
      "INFO:tensorflow:Restoring parameters from train/low_latency_conv.ckpt-12600\n",
      "INFO:tensorflow:Restoring parameters from train/low_latency_conv.ckpt-12600\n",
      "s (98, 43) (98, 43, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_size = (98, 43, 1)\n",
    "f_size = (None, 98, 43, 1)\n",
    "\n",
    "\n",
    "\n",
    "processed_images = tf.placeholder(tf.float32, shape=f_size)\n",
    "model = models.create_model(processed_images, model_settings,\n",
    "                            model_architecture='low_latency_conv', is_training=False)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "def transform_img_fn(path_list):\n",
    "    out = []\n",
    "    for f in path_list:\n",
    "        image = wav_to_features(f).squeeze()\n",
    "        s = (image.shape[0], image.shape[1], 3)\n",
    "        fake_rgb = np.ndarray(shape=s, dtype=image.dtype)\n",
    "        print('s', image.shape, fake_rgb.shape)\n",
    "        fake_rgb[:,:, 0] = image\n",
    "        fake_rgb[:,:, 1] = image\n",
    "        fake_rgb[:,:, 2] = image\n",
    "        i = tf.convert_to_tensor(fake_rgb) \n",
    "        out.append(i)\n",
    "    return session.run([out])[0]\n",
    "\n",
    "#tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "checkpoint = 'train/low_latency_conv.ckpt-12600'\n",
    "\n",
    "models.load_variables_from_checkpoint(session, checkpoint)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "saver.restore(session, checkpoint)\n",
    "\n",
    "probabilities = model\n",
    "def predict_fn(images):\n",
    "    images = [ np.expand_dims(img[:,:,0], axis=-1) for img in images ]\n",
    "    return session.run(probabilities, feed_dict={processed_images: images})\n",
    "\n",
    "\n",
    "p = predict_fn(transform_img_fn(['../../data/speech_dataset/left/122c5aa7_nohash_0.wav']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s (98, 43) (98, 43, 3)\n"
     ]
    }
   ],
   "source": [
    "images = transform_img_fn([\n",
    "    '../../data/speech_dataset/down/122c5aa7_nohash_0.wav'\n",
    "])\n",
    "p = predict_fn(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'down'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_processor.words_list[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation time:  0.7265348434448242\n"
     ]
    }
   ],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "start_time = time.time()\n",
    "# Hide color is the color for a superpixel turned OFF.\n",
    "# Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels\n",
    "image = images[0]\n",
    "explanation = explainer.explain_instance(image, predict_fn, top_labels=5, hide_color=None, num_samples=1000)\n",
    "print('explanation time: ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81e558df28>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "temp, mask = explanation.get_image_and_mask(1, positive_only=True, num_features=5, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
